{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVMatch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Fs2DLhsMMS"
      },
      "source": [
        "# Baixando os pré-requisitos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HfpUczDwhAO"
      },
      "source": [
        "!pip install pdfplumber\n",
        "import pdfplumber\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCagS42byatr"
      },
      "source": [
        "# Configurando a leitura do arquivo de vagas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF_up-QpeQKY"
      },
      "source": [
        "vagas = pd.read_excel('vagas.xlsx', sheet_name = None)\n",
        "n_vagas = len(vagas.keys())\n",
        "nome_vagas = list(vagas.keys())\n",
        "vagas = [vagas[nome_vagas[i]] for i in range(n_vagas)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfOb8kY6rztT"
      },
      "source": [
        "# Declarando a função da aplicação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4WZays3dk44"
      },
      "source": [
        "def CVMatch(cv, vaga, limite = 5):\n",
        "\n",
        "    arquivoPDF = pdfplumber.open(cv)\n",
        "    primeira_pagina = arquivoPDF.pages[0]\n",
        "    textoCRU = primeira_pagina.extract_text()\n",
        "\n",
        "    lista_de_palavras = nltk.tokenize.word_tokenize(textoCRU)\n",
        "    lista_de_palavras = [palavra.lower() for palavra in lista_de_palavras]\n",
        "    pontuacao = ['(',')',';',':','[',']',',','–','(',')','/','|','-','%','@']\n",
        "    stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "    keywords = [palavra for palavra in lista_de_palavras if not palavra in stop_words and not palavra in pontuacao]\n",
        "    textocv = \" \".join(s for s in keywords)\n",
        "\n",
        "    pesos = list(vaga['pesos'])\n",
        "    palavras_chaves = list(vaga['palavras-chave'])\n",
        "\n",
        "    cont = [textocv.count(pc) for pc in palavras_chaves]\n",
        "\n",
        "    def aux(x, limite):\n",
        "        return x if x <= limite else limite\n",
        "\n",
        "    cont = [aux(i, limite) for i in cont]\n",
        "\n",
        "    pmax = np.sum(np.array(pesos) * limite) \n",
        "\n",
        "    score = ((np.array(cont) * pesos).sum()/pmax).round(2)\n",
        "\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwO-__wvr7Da"
      },
      "source": [
        "# Inserindo curriculos e obtendo a pontuação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTe7a3IzeFjN"
      },
      "source": [
        "curriculo = ['/caminho/arquivo1.pdf',\n",
        "             '/caminho/arquivo2.pdf']\n",
        "pessoas = [[CVMatch(cv, vaga) for vaga in vagas] for cv in curriculo]\n",
        "nomes = [cv.split('/')[-1].split('.')[0] for cv in curriculo]\n",
        "matchs = pd.DataFrame(pessoas, columns = nome_vagas, index = nomes)\n",
        "matchs.sort_values(by = 'dev_back', ascending = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}